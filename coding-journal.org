#+title: Representing music with a computer
#+author: Johannes Keller
#+startup: overview
#+eval: yes

* Before I begin
I decided to write this journal to keep track of my journey towards a
representation of music in a computer. In my professional work as a
musician and musicologist I often came across the need to represent a
musical score in a digital format. I heavily used the Sibelius
notation software for many years, transcribing several complete operas
for theaters and orchestras. Then I switched to Dorico and I was very
pleased with the concise concept and the high quality engraving. After
having notated again several large scale works I changed my digital
life to a more open source oriented approach, moved from Mac to Linux
and faced the dilemma: should I keep a Windows partition on my
ThinkPad, just to run Dorico? After some time with this hybrid setup I
became tired and I moved over completely to Arch Linux, where I fully
embraced the concept of plain text. After two years using Vim as my
primary interface to my computer I moved into Emacs. Currently my
professional and private life is mainly coordinated within org mode,
including the writing of my dissertation. For music engraving I use
mainly Lilypond.

Notation software is just one part of my use cases. The other part are
various experiments and studies concerning polyphonic music of the
16th century. I have a particular interest in intonation phenomena. My
daily work with highly microtonal keyboard instruments from the 16th
century sharpened my senses for the specific properties of musical
intervals and the musical diversity that can be gained from adopting
complex concepts of pitch organisation. In that context I was given
the task of transcribing all musical examples in Nicola Vicentinos
treatise /L'antica musica ridotta alla moderna prattica/, printed in
Rome in 1555. These roughly 350 snippets of very strange music needed
to encoded digitally. Because of the unusual pitch system, this
notation wasn't compatible with any established standard of music
encoding. I decided to create my own way of representing these scores
and I wrote a program to transform this encoding into various
graphical formats, including modern looking scores engraved by
Lilypond.

I'm not a programmer. To me, coding is still a mysterious island with
fascinating discoveries, but also many forbidden fruits. As a musician
I learned to trust my gut feeling. And my guts tell me that there is
something to be discovered in the field of digital representation of
musical structure. In many occasions over the last couple of years I
implemented various approached for rudimentary tools that had the
purpose of generating an explicative figure for a journal article or
they were used to control an analog synthesizer by modelling specific
tuning systems. The life span of those mini projects was never longer
than a couple of days or weeks.

I feel that it is time to invest some time and effort to dig deeper
and potentially come up with something that could be used for many
projects, maybe even by more than just me.

So, here we go.

* The environment
I decided to use Common Lisp. The reason is simple: it's my personal
preference. I used to code in C++ in high school, but stopped
programming entirely during my musical studies. Only a couple of years
ago I picked it up again, after a phase of being obsessively
interested in life coding. Andrew Sorensen with his Impromptu /
Extempore languages were a huge inspiration. Because of his work I
taught myself Scheme and used it for a couple of musical
performances. Orm Finnendahl pointed me towards Common Lisp, which
became the main language for my every day needs. I never wrote a
finished piece of software, but dozens of small helper programs.

My understanding of Common Lisp is far from complete, I read the first
half of SICP, worked through /Practical Common Lisp/ and read many
blogs. I have a hunch that some properties of the Lisp language will
be very important for my undertaking, particularly its homoiconicity
and the distinction of compile time and execution time.

Apart from those reasons I perceive Lisp as a poetic language. There
are always many different paths that lead to a solution. Having many
ways to express something is a critical ingredience for a poetic
language. I also feel that it encourages quick, dirty and intuitive
coding. Planning, setting goals and checkpoints destroy any kind of
artistic approach, in my personal experience. Child like play needs to
be possible, and Lisp seems to allow that. Plus, its syntax is very
compact, I can express complicated structures with few key strokes,
which is an important feature when it comes to time critical
coding. Interacting with your code while it is generating music has a
great potential for stress. Having a compact language that is
efficient to edit helps.

Coming from an Early Music background, I appreciate old things. The
fact that Common Lisp is a language with a standard has a comforting
quality because it's easier to trust that code will be executable over
a long period of time.

For this journal I'm going to use Emacs's /org mode/ and Common Lisp
code blocks. I use SBCL as a Lisp instance. For now I don't write any
code anywhere else.

Let's test it. I started SLIME with 'M-x slime' (normally I use 'sly'
for Common Lisp development, but org mode seems to work better with
SLIME).

#+begin_src lisp
3/2
#+end_src

#+RESULTS:
: 3/2

I need to understand how to connect separate code blocks to one
context. Let's test this as well.

#+begin_src lisp
(defun simplify (interval &optional (identity-interval 2/1))
  (cond ((< interval 1)
         (simplify (* interval identity-interval) identity-interval))
        ((>= interval identity-interval)
         (simplify (/ interval identity-interval) identity-interval))
        (t interval)))
#+end_src

#+RESULTS:
: SIMPLIFY

I didn't execute this definition, but I'd like to use the function in
the following code block.

#+begin_src lisp
(simplify 9/4)
#+end_src

#+RESULTS:
: 9/8

Ok, this sends me into the debugger saying that 'simplify' is unbound.

After executing the code block with the function definition, I can call
it in the other code block.

Now what happens if I quit SBCL and restart it?

I need to manually execute all code blocks again.

First I try to avoid the explicit confirmation to execute a code
block, according to
https://orgmode.org/manual/Evaluating-Code-Blocks.html.

#+begin_src lisp :eval yes
2/1
#+end_src

#+RESULTS:
: 2

Ok, that works. Would be great to be able to enable this globally. I
added it at the top of the file.

#+begin_src lisp
5/4
#+end_src

#+RESULTS:
: 5/4

Great, this works. I discovered that ~org-babel-execute-buffer~ (C-c
C-v C-b) executes the whole document. That might be sufficient. It
causes 'redefining' warnings in the REPL, but that seems to be ok for
now.

* Music as a two-dimentional phenomenon
The most naive approach to representing music with data structures is
to treat each note as a point in a two dimensional space. The
horizontal axis would be time, the vertical axis pitch.

#+begin_src lisp
(defstruct note
           time-coordinate
           pitch-coordinate)

(defun note (time pitch)
  (make-note :time-coordinate time :pitch-coordinate pitch))
#+end_src

#+RESULTS:
: NOTE

A simple melody could be encoded as a collection of note objects.

#+begin_src lisp
(defvar *ave* (list (note 0 1/1) (note 1 3/2) (note 2 8/5) (note 3 4/3) (note 4 3/2)))

,*ave*
#+end_src

#+RESULTS:
: (#S(NOTE :TIME-COORDINATE 0 :PITCH-COORDINATE 1)
:  #S(NOTE :TIME-COORDINATE 1 :PITCH-COORDINATE 3/2)
:  #S(NOTE :TIME-COORDINATE 2 :PITCH-COORDINATE 8/5)
:  #S(NOTE :TIME-COORDINATE 3 :PITCH-COORDINATE 4/3)
:  #S(NOTE :TIME-COORDINATE 4 :PITCH-COORDINATE 3/2))

Obviously, there is a lot to improve. But this extremely rudimentary
implementation shows the necessity for a fundamental decision: Am I
going to represent the appearance of a musical score in modern
notation or the mental model of musicians when they read, process or
invent musical structures?

In order to make a decision of this magnitude, I'll examine both
approaches a bit further.

* Representing a score
** A naive sketch
Conventional musical notation depends on many conventions. I try to
summarize them in order to design a suitable data structure,
maintaining the two dimensional space as a starting point. Both the
time and the pitch dimension are quantized, which means that there is
a predefined set of discrete values for both coordinates.

Music consists of individual notes, represented by glyphs. Their
graphical properties encode the duration of the note (the shape of the
note head, possibly modified with a rhythmical dot, the presence or
absence of a stem, with or without flags). The glyphs are printed
sequentially, there is no absolute time code involved, the note values
are read in an additive way. This series of note values can be
structured in beats and bars, various durations of beats and bars can
be globally defined (time signature) and locally changed (signature
change).

~note-value~ contains all the semantically relevant information of a
note glyph. There are many more properties of a note glyph, like the
direction of the stem, the length and thickness of the stem, etc., but
these don't change the musical meaning of the glyph. The following
struct doesn't include any time context, like the time signature, or
place within a bar or beat.

#+begin_src lisp
(defstruct note-value
           "NOTEHEAD-SHAPE: :empty, :filled. FLAG: integer from 0 to 4."
           notehead-shape
           stemp
           flag)

(defun note-value (shape stemp flag)
  "Creates an instance of the struct NOTE-VALUE."
  (make-note-value :notehead-shape shape :stemp stemp :flag flag))
#+end_src

#+RESULTS:
: NOTE-VALUE

Pitch is encoded by the vertical position of the note heads on a line
system. The horizontal lines represent an ordered succession of
pitches, called the gamut. The positions on the gamut are labelled
with the letters A-G. The pitch that is defined by the position of the
note head can be modified by using alteration signs (sharps and flats)
placed left of a note head. They can also be applied globally by
placing them at the beginning of a line system.

~pitch~ contains the semantically relevant information for a notes
pitch. It doesn't take into account any global alterations.

#+begin_src lisp
(defstruct pitch
  "LETTER: chars \\#a to \\#g. OCTAVE: integer, 0 for central octave. ALTERATION: :sharp, :flat or
:natural."
  letter
  octave
  alteration)

(defun pitch (letter octave alteration)
  (make-pitch :letter letter :octave octave :alteration alteration))
#+end_src

#+RESULTS:
: PITCH

With this, I can express a melody as a list of ~note~-objects, using
~note-value~ and ~pitch~ as coordinates.

#+begin_src lisp
(defvar *follia* (list (note (note-value :empty t nil) (pitch #\d 0 nil))
                       (note (note-value :filled t nil) (pitch #\c 0 :sharp))
                       (note (note-value :empty t nil) (pitch #\d 0 nil))
                       (note (note-value :filled t nil) (pitch #\e 0 nil))
                       (note (note-value :empty t nil) (pitch #\f 0 nil))))
#+end_src

#+RESULTS:
: *FOLLIA*

** Lilypond output
The next step is to figure out what this representation can be used
for. There are two obvious cases: generate a human readable graphical
output, like a proper score, or generate an audio signal. These are
two very different tasks. To produce a score is easy, since the data
structure contains the relevant properties of notes. The data is
modeled after a musical score, so the conversion into a graphical
representation of a score is trivial, for example using Lilypond. For
each property there needs to be a dictionary in order to translate the
data into valid Lilypond syntax.

#+begin_src lisp
(defparameter *dict-alterations* '((:sharp . "is")
                                  (:flat . "es")
                                  (:natural . "")))

(defun lookup-alteration (alteration)
  (if alteration
      (cdr (assoc alteration *dict-alterations*))
      ""))

(defparameter *dict-octaves* '((-2 . ",")
                               (-1 . "")
                               (0 . "'")
                               (1 . "''")
                               (2 . "'''")))

(defun lookup-octave (octave)
  (cdr (assoc octave *dict-octaves*)))

(defparameter *dict-values* '(((:empty nil nil) . "1")
                              ((:empty t nil) . "2")
                              ((:filled t nil) . "4")
                              ((:filled t 1) . "8")
                              ((:filled t 2) . "16")))

(defun lookup-value (note-value)
  (cdr (assoc (list (note-value-notehead-shape note-value)
                    (note-value-stemp note-value)
                    (note-value-flag note-value))
              ,*dict-values*
              :test #'equal)))

(defun convert-glyph-to-lilypond (glyph)
  (concatenate 'string (string (pitch-letter (note-pitch-coordinate glyph)))
                       (lookup-alteration (pitch-alteration (note-pitch-coordinate glyph)))
                       (lookup-octave (pitch-octave (note-pitch-coordinate glyph)))
                       (lookup-value (note-time-coordinate glyph))
                       " "))

(defparameter *lilypond-header*
  "\\version \"2.22.2\"
\\paper{indent=0\\mm tagline=##f line-width=170\\mm}
#(ly:set-option 'use-paper-size-for-page #f)
#(ly:set-option 'tall-page-formats 'png)

{ \\override Score.TimeSignature.stencil = ##f
  \\cadenzaOn")

(defparameter *lilypond-footer*
   "}")

(defun data-to-lilypond (data)
  (let ((result (format nil "~a~% " *lilypond-header*)))
    (dolist (glyph data)
      (setf result (concatenate 'string result (convert-glyph-to-lilypond glyph))))
    (concatenate 'string result *lilypond-footer*)))

(data-to-lilypond *follia*)
#+end_src

#+RESULTS:
: \version "2.22.2"
: \paper{indent=0\mm tagline=##f line-width=170\mm}
: (ly:set-option 'use-paper-size-for-page #f)
: (ly:set-option 'tall-page-formats 'png)
:
: { \override Score.TimeSignature.stencil = ##f
:   \cadenzaOn
:  d'2 cis'4 d'2 e'4 f'2 }

The results can then be processed by Lilypond, which produces the
score as expected. I tweaked the lilypond source with some settings to
create a small snippet and not a whole page of music, and to suppress
the time signature and bar lines.

#+begin_src lilypond :file follia.png
\version "2.22.2"
\paper{indent=0\mm tagline=##f line-width=170\mm}
#(ly:set-option 'use-paper-size-for-page #f)
#(ly:set-option 'tall-page-formats 'png)

{ \override Score.TimeSignature.stencil = ##f
  \cadenzaOn
 d'2 cis'4 d'2 e'4 f'2 }
#+end_src

#+RESULTS:
[[file:follia.png]]

Obviously there is lots of room for improvements. But conceptually
this conversion is only a matter of a straight forward translation.

** Processing pitch information
*** Tuning functions and Tonsystems
To produce an audio output based on the data structure is more
challenging. One part of the challenge is the technical solution how
to produce audio, which involves some sort of digital signal
processing. The other part is how to interpret the score. Obviously
the data is of a relative nature. Pitch and time are represented by
symbols, not by absolute values for frequency in Hz and the number of
elapsed milliseconds or something like that.

The challenge of the signal processing will be postponed and replaced
by a visual representation: Pitch and time should be calculated in a
way that they could be fed to an audio engine, but instead of an
actual audio processor I'll feed them to a graphical backend that will
produce a two dimensional diagram displaying time on the x axis and
frequency on the y axis.

I'll focus first on the tranformation of the musical data into
absolute values, and on the graphical output later. This
transformation requires some music theoretical ground rules. Musical
notation (my data structure) implies quite a bit of music theoretical
knowledge. The mapping between pitch letters with their alterations to
frequencies is defined by a tuning system, or scale.

#+begin_src lisp
(defparameter *frequency-table* '(((#\a nil 0) . 440.0)
                                  ((#\a :sharp 0) . 466.1638)
                                  ((#\b :flat 0) . 466.1638)
                                  ((#\b nil 0) . 493.8833))
              "The keys are lists containing the three elements of the PITCH struct.")
#+end_src

#+RESULTS:
: *FREQUENCY-TABLE*

I'd like to keep this flexible, so instead of this kind of dictionary
I'll express the scale as a function. That allows me to experiment
with different function definitions and generate different outputs
accordingly. To write this function, I need to implement a model of a
tuning system. There are various models I can think of (for sure I'll
discover many more):

- Linear system :: Pitches are generated by creating a chain of always
  the same interval (the generator). All members of the chain are
  reduced in a way that they lie within the identity interval,
  normally the octave. A sorted version of such a list of intervals
  represents the pitches that are available in a scale. I used linear
  systems to desribe pythagorean and regular meantone systems.
- Equal division :: By dividing a specific interval (normally the
  octave) in a specific number of equal parts you can describe the
  available pitches of a scale. Subsets can be defined. This is the
  obvious model for the piano tuning (twelve equal parts per octave,
  12-EDO or 12ed2), or meantone approaches such as 19-ed2 and 31ed2.
- Lattice (Tonnetz) :: An n-dimensional lattice where each axis
  represents a specific interval size. This approach is very
  productive for Just Intonation systems.

I'll start with an implementation of linear systems. The core of the
model is a function that handles the chain of intervals.

#+begin_src lisp
(defun linear-system (index &optional (generator-interval 3/2) (identity-interval 2/1))
  (if (zerop index)
      1/1
      (* generator-interval (linear-system (decf index) generator-interval identity-interval))))

;; c - g - d - a - e
;; 0   1   2   3   4
(linear-system 4)
#+end_src

#+RESULTS:
: 81/16

This is a recursive approach, since I know that I will need to add a
function to move the members of the chain into the range of the
identity interval. As the example above shows, ~(linear-system 4)~
doesn't produce the Pythagorean third, but the Pythagorean third plus
two octaves. For the function to reduce an interval to its smallest
form I also follow a recursive approach.

#+begin_src lisp
(defun interval-modulo (interval &optional (identity-interval 2/1))
  (cond ((< interval 1/1)
         (interval-modulo (* interval identity-interval) identity-interval))
        ((>= interval identity-interval)
         (interval-modulo (/ interval identity-interval) identity-interval))
        (t interval)))

(interval-modulo 81/16)
#+end_src

#+RESULTS:
: 81/64

I could either apply the requested "modulo" function to the end result
of ~linear-system~, when breaking the recursive loop, or in each
recursive call. I will opt for the latter, since it will prevent the
build up of ratios with very large numerators and denominators. This
doesn't seem very efficient, but optimisation can be dealt with later.

A side node concerning efficiency: if any musical data (like a scale
in this moment) needs to be calculated only once, and then played
back, efficiency can be neglected, because it is not a musically
critical real time situation. But if I'd like to interfere with a
running system while it is playing back a score, such calculations
might happen at sample rate, for example to morph gradually between
two tuning systems while playing. In this case, the calculation of a
scale becomes part of the audio design pipeline and needs to be as
efficient as possible.

The previous implementation of ~linear-system~ didn't process negative
indices. A negative index is convenient to describe a chain of fifths
starting from a central note with index 0. Negative indices cause the
chain to expand "to the left", adding descending fifths to the system.

#+begin_src lisp
(defun linear-system (index &optional (generator-interval 3/2) (identity-interval 2/1))
  (if (zerop index)
      1/1
    (interval-modulo (* (if (minusp index) (/ 1 generator-interval) generator-interval)
                        (linear-system (if (minusp index)
                                           (incf index)
                                         (decf index))
                                       generator-interval
                                       identity-interval))
                     identity-interval)))

(linear-system 4)
#+end_src

#+RESULTS:
: 81/64

This seems to work. To collect the pitches of a conventional
Pythagorean tuning into a lookup table, I just need to decide the
range of the chain of fifths. For example:

|  E♭ |  B♭ |  F |  C |  G | D | A | E | B♮ | F♯ | C♯ | G♯ |
| -5 | -4 | -3 | -2 | -1 | 0 | 1 | 2 | 3 | 4 | 5 | 6 |

#+begin_src lisp
(defun generate-pythagorean-dictionary (index-start index-end)
  (mapcar (lambda (index)
            (cons index (linear-system index)))
          (loop for i from index-start to index-end collect i)))

(defparameter *pythagorean-scale* (generate-pythagorean-dictionary -5 6))

(defun lookup-pythagorean-pitch (index)
  (cdr (assoc index *pythagorean-scale*)))

(lookup-pythagorean-pitch 4)
#+end_src

#+RESULTS:
: 81/64

With this, I can easily generate dictionaries for any linear system,
including regular meantone temperaments. I just need to adjust the
generator interval and the range of fifths. But there is still one
piece missing. ~linear-system~ expects an index, but the pitch
information (~pitch-coordinate~) in my score data consists of
~letter~, ~octave~ and ~alteration~.

There is still one translation process necessary to convert a
~pitch-coordinate~ into an index for a linear system. It is already
clear that this translation depends on the implementation of the
model. For the linear system, an index is required that represents the
position of the note within the interval chain. For an equal division
function, the expected index would represent something else, probably
the number of atomic intervals the requested interval consists of. For
a Tonnetz, the index would probably be an n-dimensional coordinate
that describes the position of the pitch within the lattice.

That means that the translation between a ~pitch-coordinate~ and the
arguments that the function of the model of the tuning system expects
depends on the model itself. Therefore I need to add this translation
to the ~linear-system~ function. But strictly speaking it's not part
of the tuning system. I decide to introduce the concept of a
Tonsystem, which is more like a naming convention to encode pitch
information in a way that is meaningful to the tuning function.

This creates the chain

| note name (~pitch-coordinate~) | ➙ | Tonsystem | ➙ | tuning model | ➙ | output module |

The Tonsystem is a function that takes a ~pitch-coordinate~ (a note
name represented by ~letter~, ~octave~ and ~alteration~) and
transforms it to an index that is meaningful in the context of a
specific Tonsystem (like the index of a linear system). The tuning
model takes this index and transforms it into a number that describes
the pitch (like the ratios in the Pythagorean tuning example above,
calculated by ~linear-system~). This number is still relative to a
reference frequency that is not defined by the tuning function. It can
then be used by an output module to calculate a sounding frequency or
the y-coordinate in a diagram.

This reminds me of an OpenGL pipeline. You describe your virtual world
with data structures, which are then transformed by various functions
(shaders) to finally get to the numbers that are required to draw
something on the screen.

*** The pitch pipeline
**** Aspects of the pitch pipeline
It would be interesting to come up with a generic implementation of
such a pipeline. Later I could add implementations of various
Tonsystems, tuning models and output modules to represent different
approaches how to transform musical data into actual sound.

The pipeline could be fed at any stage. To process conventional
scores, for example in the case of notated historical music, it would
start at the first stage. All it would need is a parser for existing
music encoding standards such as MusicXML, MEI, Lilypond or Humdrum,
and existing music could be sent through the pipeline. But I expect
that it could be artistically interesting to inject or modify data at
later stages, for example to provide musical information directly to
the tuning function, without bothering about the conventional
notation. That would enable me to compose beyond the restrictions of
conventional music notation (in a way, collections of indices for
linear systems are just another kind of notation).

The real time manipulation of all the stages of the pipeline could be
an important feature. It would allow to morph existing musical content
while it is being rendered. Morphing within a tuning model is trivial,
as long as the pipeline is being updated at sample rate: I'd just need
to update the size of the generator interval of the linear system. To
morph between different tuning models will pose a challenge, since the
underlying data is not compatible.

To sketch an implementation of this pitch pipeline I'll invent
concrete scenarios and perform case studies.

**** Case study: polyphonic motet in meantone temperament
The data is somehow provided as a succession of notenames per voice,
maybe after parsing a MusicXML file. The time dimension is ignored for
this case study, since I'm focusing on pitch only. The piece requires
more than 12 note names per octave (E♭ and D♯, G♯ and A♭). The note
names need to be translated into indices that represent a location on
the chain of fifths. This requires some sort of dictionary that maps
every possible note name to indices. There are various options:

- Only 12 indices are defined, D♯ and E♭ are mapped to the same
  index. This makes sense if a keyboard with 12 keys per octave should
  be simulated. The very fact that there is no distinction between E♭
  and D♯ creates interesting musical situations ('impossible' notes
  sound badly out of tune, which might be used expressively).
- All note names that can be expressed are mapped to indices. This
  makes sense if a keyboard with more than 12 keys per octave should
  be simulated. Or some approximation of keyboard-independent
  intonation, like a violin consort for example.

In this example, the dictionary always works with the same principle,
but there should be various versions available that use different
'ranges' in their mapping procedure. When setting up the pipeline it
would be necessary to decide for a specific version of the dictionary.

The next stage in the pipeline is the tuning function, which also need
configuration. Meantone temperaments can be modeled as linear
systems. Obviously the size of the fifth needs to be
defined. Alternatively it could also be defined as an equal division
of the octave, with 19 for 1/3-comma-meantone, 31 for
1/4-comma-meantone and 55 for 1/6-comma-meantone. Furthermore,
irregular setups might be interesting, which need to be custom
defined, for example by a list of the sizes of the fifths. When
setting up the pipeline, a specific tuning model needs to be selected
and configured by defining the necessary parameters of the model.

The final stage is the output model which defines the appearance of
the final product (audio stream, audio file, some kind of
diagram). It also needs to be configured, in the case of audio
production it needs a reference pitch and possibly some parameters for
the sound synthesis.

These three stages could be implemented with an object oriented
approach. Each stage would be a class. Subclasses could represent the
various variations, for example different tuning models. They would be
configured by creating instances with specific initialisation
arguments. When creating a pipeline, I'd need to pass in instances of
the stages. Ideally it would be possible to reconfigure instances
while the pipeline is in use, or even swap stages with other
instances.

The pipeline would be represented by another class, holding the
instances of the stages. A method of this class would manage the data
flow from stage to stage. It would also be handling real time
modifications of the pipeline.

label:idea-dsl A side note and idea for the future: The configuration
and manipulation of the pipeline could be accessed with a domain
specific language. This would offer the possibility that users who are
not familiar with the implementation of the whole system could use it,
by just learning this language. I could optimize the language for
brevity, which would make it interesting for live coding. Furthermore
it would simplify the process of mapping it to hardware
controllers. It could be artistically interesting to have a hardware
knob or slider to gradually change the generator interval during a
piece, for example.

**** Case study: Serbian songs
Two-part serbian songs in an unknown tonal system should be
simulated. The intervals can be expressed with a three-dimensional
Tonnetz, with an axis for pure fifths, one for pure major thirds and
one for natural sevenths.

The first stage will be omitted, since the pitch information is
available directly as coordinates of the Tonnetz. The tuning function
consists of a Tonnetz that needs to be configured accordingly when
setting up the pipeline.

label:idea-stage-visualisation Idea for the future: Each stage could
have its own real time visualisation. Particularly the Tonnetz model
could be interesting to watch while pitch information is being
processed. In polyphonic setups each voice would need its own
visualisation.

label:idea-reverse-pipeline Idea for the future: It could be useful to
have the option of using a stage in the pipeline in reverse. In this
case the first stage could produce a transcription of the
Tonnetz-based primary data into staff notation, using for example the
Helmholtz-Ellis notation, or standard notation with detune-information
in cent. Having reversible stages would require that each
transformation method is defined bidirectionally. Reversing raw pitch
information (ratios or frequencies) could be transformed into
positions in a Tonnetz or locations in a linear system by finding best
approximates. This function could be highly useful for theoretical
explorations. It could create transcriptions or snapshots of musical
situations that are being created by freely modulating pitch, for
example by turning hardware knobs by ear. Another application of
reversible stages are complex transposition tasks. It could be
studied to what extend it is possible to transpose a melody within the
Harry-Partch-system by calculating specific frequencies, shifting
them, and sending them back through the pipeline.

**** Datatypes
An object oriented approach seems sensible to represent the pipeline
and its stages. I'm not sure about the data that flows between the
stages. It might be useful to define the format of the data in a
precise way to make data checking as transparent as possible. Since
there will be many similar but slightly different data formats, it
might be challenging to keep track.

The first case study above showed that the validity of the output of
the first stage depends on the definition of the dictionary. If it
represents a keyboard with 12 keys per octave, there can only be 12
different indices. But the tuning function theoretically accepts any
index, since linear systems are infinite by definition. It probably
makes sense to implement a data checking method for each stage
subclass. There will be a data checking method for the incoming data
(for example the validity of note name information) and a data
checking method for the outgoing data (for example the validity of
indices).

It might makes sense to represent different data formats by classes
themselves, since they could exist independently of specific
stages. I can think of these examples of note naming conventions:

- Standard note names with none or one chromatic alteration.
- Standard note names with none, single or multiple alterations
  (double sharps, triple flats, etc.).
- Quarter tone notation (with quarter-sharp, three-quarter-flat, etc.).
- Vicentino notation (with single alterations and enharmonic dots).
- Helmholtz-Ellis notation.

If the fist stage simulates a keyboard with 19 keys per octave and
outputs indices for linear systems, it might be desireable to use it
for different note naming conventions. That means that the processing
method of this stage needs to distinguish between different
notations. This could be solved by multiple dispatch
methods. For each accepted notation convention there would be a
separate processing method, applying the data format checking method
provided by the notation class.

That means I could define a class for pitch information, with
subclasses for /note names/, /Tonnetz indices/, /interval ratios/,
etc. For each specific format (like /Vicentino notation/) there would
be a subclass of the /note names/ class, with a dedicated
implementation of the format checking method, or with a constructor
method that only allows the creation of valid instances. Each pitch
information that enters the pipeline would be an instance of a pitch
information subclass. In each stage of the pipeline there would be a
transformation from one pitch information subclass into another one.

An alternative approach would be to define custom types. This could
make type checking more flexible and I'd have the choice to check
types at compile time more rigourously. A class based approach has the
danger of triggering run time errors that would stop the signal
processing, which is unacceptable in a real time environment. This
danger might be reduced by using the condition system, which I still
know only superficially.

At this point I like the idea of having instances of finely defined
classes for any kind of pitch information. I imagine that it will help
my own mental transparency. I'll be working on this system for a long
time, and it will be crucial to have clean procedures to add new data
formats much later.

Having classes to represent notation conventions also offers the
possibility of conversion methods between them. It could be useful to
have conversions between /double accidentals/ and /Vicentino
notation/. Further subclasses might even offer the possibility of
defining the available note names on very specific instruments, for
example a keyboard instrument with a short octave could be represented
by a subclass of /standard notation/.

A class based approach to represent pitch information can potentially
also solve another problem that I ignored so far: the handling of
octaves. Some pitch information formats are octave sensitive, other
are not (in that case they encode pitch classes, like indices in
linear systems). It is vital that original octave information is
preserved thoughout the pipeline, also by stages that don't care about
it. This could be done by having a dedicated slot for the octave
information that is carried along through all stages.

**** Implementing pitch data, first sketch
The following class hierarchy is a first attempt.

#+begin_src lisp
(defclass pitch-data ()
  ((octave :accessor octave
           :documentation "NIL: treated as pitch class, NUMBER: octave indicator, LIST of NUMBERs: simultaneous octaves"))
  (:documentation "Parent class of subclasses for any kind of pitch data."))

(defclass note-name (pitch-data)
  ((letter :accessor letter
           :documentation "CHAR describing the root note name.")
   (accidental :accessor accidental
               :documentation "KEYWORD describing an accidental, NIL for the raw root name."))
  (:documentation "Parent class for note names based on letters and alterations."))

(defclass note-name-12 (note-name)
  ()
  (:documentation "Operational subclass for note names represented by a standard keyboard with B♭, E♭ and F♯, C♯,
G♯. All other alterations are accepted as input but silently mapped onto these note names."))

(defclass note-name-smn (note-name)
  ()
  (:documentation "Operational subclass for note names that are part of standard music notation (multiple alterations
are accepted)."))

(defclass note-name-vicentino (note-name)
  ((enharmonic-dot :accessor enharmonic-dot
                   :documentation "NIL if absent, :dot when raised by a 'diesis enarmonico minore', :comma when raised by a 'comma'."))
  (:documentation "Operational subclass for note names of Vicentino's staff notation."))

(defclass note-name-arciorgano (note-name-vicentino)
  ()
  (:documentation "Operational subclass representing the available notes of the Basel Arciorgano."))

(defclass key-name-vicentino (pitch-data)
  ((letter :accessor letter
           :documentation "CHAR a-g.")
   (ordine :accessor ordine
           :documentation "NUMBER 1-6."))
  (:documentation "Operational subclass for key names according to Vicentinos L'antica musica (Rome 1555)."))

(defclass key-name-arciorgano (key-name-vicentino)
  ()
  (:documentation "Operational subclass representing the available keys of the Basel Arciorgano."))
#+end_src

#+RESULTS:
: #<STANDARD-CLASS COMMON-LISP-USER::KEY-NAME-ARCIORGANO>



* Representing musical thinking

* Ideas for the future
- ref:idea-dsl
- ref:idea-stage-visualisation
- ref:idea-reverse-pipeline
